  0%|          | 0/1688 [00:00<?, ?it/s]/home/ywu19/anaconda3/envs/less/lib/python3.10/site-packages/transformers/data/data_collator.py:646: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:274.)
  batch["labels"] = torch.tensor(batch["labels"], dtype=torch.int64)
  0%|          | 1/1688 [00:04<1:58:37,  4.22s/it]
{'loss': 3.3613, 'grad_norm': 6.255621910095215, 'learning_rate': 3.921568627450981e-07, 'epoch': 0.0}


  0%|          | 3/1688 [00:11<1:46:18,  3.79s/it]
{'loss': 3.6675, 'grad_norm': 12.160858154296875, 'learning_rate': 1.1764705882352942e-06, 'epoch': 0.01}

  0%|          | 4/1688 [00:15<1:51:51,  3.99s/it]


  0%|          | 6/1688 [00:23<1:50:15,  3.93s/it]

  0%|          | 7/1688 [00:27<1:50:25,  3.94s/it]

  0%|          | 8/1688 [00:31<1:49:23,  3.91s/it]

  1%|          | 9/1688 [00:34<1:46:05,  3.79s/it]

  1%|          | 10/1688 [00:38<1:46:47,  3.82s/it]

  1%|          | 11/1688 [00:42<1:45:54,  3.79s/it]

  1%|          | 12/1688 [00:46<1:51:02,  3.98s/it]

  1%|          | 13/1688 [00:50<1:50:17,  3.95s/it]

  1%|          | 14/1688 [00:54<1:47:12,  3.84s/it]

  1%|          | 15/1688 [00:58<1:47:51,  3.87s/it]

  1%|          | 16/1688 [01:02<1:48:12,  3.88s/it]
{'loss': 2.4035, 'grad_norm': 5.721335411071777, 'learning_rate': 6.274509803921569e-06, 'epoch': 0.04}

  1%|          | 17/1688 [01:06<1:47:03,  3.84s/it]


  1%|          | 19/1688 [01:13<1:46:01,  3.81s/it]

  1%|          | 20/1688 [01:17<1:47:07,  3.85s/it]

  1%|          | 21/1688 [01:21<1:46:48,  3.84s/it]

  1%|▏         | 22/1688 [01:25<1:47:19,  3.87s/it]

  1%|▏         | 23/1688 [01:29<1:48:51,  3.92s/it]

  1%|▏         | 24/1688 [01:32<1:45:34,  3.81s/it]

  1%|▏         | 25/1688 [01:36<1:44:10,  3.76s/it]

  2%|▏         | 26/1688 [01:40<1:45:03,  3.79s/it]
{'loss': 1.3243, 'grad_norm': 3.308046340942383, 'learning_rate': 1.0196078431372549e-05, 'epoch': 0.06}


  2%|▏         | 28/1688 [01:48<1:49:14,  3.95s/it]

  2%|▏         | 29/1688 [01:52<1:51:15,  4.02s/it]

  2%|▏         | 30/1688 [01:56<1:51:14,  4.03s/it]

  2%|▏         | 31/1688 [02:00<1:49:26,  3.96s/it]

  2%|▏         | 32/1688 [02:04<1:48:16,  3.92s/it]

  2%|▏         | 33/1688 [02:08<1:48:27,  3.93s/it]

  2%|▏         | 34/1688 [02:12<1:48:44,  3.94s/it]

  2%|▏         | 35/1688 [02:16<1:49:18,  3.97s/it]
{'loss': 1.184, 'grad_norm': 2.7065510749816895, 'learning_rate': 1.3725490196078432e-05, 'epoch': 0.08}

  2%|▏         | 36/1688 [02:19<1:45:10,  3.82s/it]


  2%|▏         | 38/1688 [02:27<1:44:49,  3.81s/it]

  2%|▏         | 39/1688 [02:31<1:46:07,  3.86s/it]

  2%|▏         | 40/1688 [02:35<1:47:39,  3.92s/it]

  2%|▏         | 41/1688 [02:39<1:49:14,  3.98s/it]

  2%|▏         | 42/1688 [02:43<1:46:04,  3.87s/it]

  3%|▎         | 43/1688 [02:47<1:46:37,  3.89s/it]

  3%|▎         | 44/1688 [02:50<1:43:05,  3.76s/it]

  3%|▎         | 45/1688 [02:54<1:43:27,  3.78s/it]

  3%|▎         | 46/1688 [02:58<1:46:27,  3.89s/it]

  3%|▎         | 47/1688 [03:02<1:45:32,  3.86s/it]
{'loss': 1.0803, 'grad_norm': 1.9555530548095703, 'learning_rate': 1.843137254901961e-05, 'epoch': 0.11}

  3%|▎         | 48/1688 [03:06<1:44:08,  3.81s/it]

  3%|▎         | 49/1688 [03:10<1:46:59,  3.92s/it]


  3%|▎         | 51/1688 [03:17<1:44:23,  3.83s/it]

  3%|▎         | 52/1688 [03:21<1:43:50,  3.81s/it]
{'loss': 1.3731, 'grad_norm': 2.520639657974243, 'learning_rate': 1.9987782529016497e-05, 'epoch': 0.12}


  3%|▎         | 54/1688 [03:29<1:47:25,  3.94s/it]

  3%|▎         | 55/1688 [03:33<1:48:13,  3.98s/it]

  3%|▎         | 56/1688 [03:37<1:45:35,  3.88s/it]

  3%|▎         | 57/1688 [03:41<1:44:41,  3.85s/it]

  3%|▎         | 58/1688 [03:45<1:46:55,  3.94s/it]

  3%|▎         | 59/1688 [03:49<1:46:05,  3.91s/it]

  4%|▎         | 60/1688 [03:53<1:45:09,  3.88s/it]

  4%|▎         | 61/1688 [03:57<1:45:45,  3.90s/it]

  4%|▎         | 62/1688 [04:00<1:42:37,  3.79s/it]

  4%|▎         | 63/1688 [04:04<1:44:24,  3.86s/it]

  4%|▍         | 64/1688 [04:08<1:44:55,  3.88s/it]
{'loss': 1.0979, 'grad_norm': 2.2065305709838867, 'learning_rate': 1.9841172877214418e-05, 'epoch': 0.15}

  4%|▍         | 65/1688 [04:12<1:45:38,  3.91s/it]


  4%|▍         | 67/1688 [04:20<1:48:15,  4.01s/it]

  4%|▍         | 68/1688 [04:24<1:47:06,  3.97s/it]

  4%|▍         | 69/1688 [04:28<1:47:19,  3.98s/it]
{'loss': 1.0515, 'grad_norm': 1.7879345417022705, 'learning_rate': 1.9780085522296886e-05, 'epoch': 0.16}

  4%|▍         | 70/1688 [04:32<1:45:04,  3.90s/it]


  4%|▍         | 72/1688 [04:40<1:48:52,  4.04s/it]

  4%|▍         | 73/1688 [04:44<1:50:52,  4.12s/it]

  4%|▍         | 74/1688 [04:48<1:48:31,  4.03s/it]
{'loss': 1.0871, 'grad_norm': 2.014228343963623, 'learning_rate': 1.9718998167379355e-05, 'epoch': 0.17}

  4%|▍         | 75/1688 [04:52<1:45:40,  3.93s/it]

  5%|▍         | 76/1688 [04:56<1:44:37,  3.89s/it]

  5%|▍         | 77/1688 [05:00<1:44:44,  3.90s/it]


  5%|▍         | 79/1688 [05:07<1:44:20,  3.89s/it]
{'loss': 1.1635, 'grad_norm': 2.1834230422973633, 'learning_rate': 1.9657910812461824e-05, 'epoch': 0.19}


  5%|▍         | 81/1688 [05:15<1:45:04,  3.92s/it]

  5%|▍         | 82/1688 [05:19<1:45:27,  3.94s/it]

  5%|▍         | 83/1688 [05:23<1:44:55,  3.92s/it]

  5%|▍         | 84/1688 [05:27<1:46:52,  4.00s/it]
{'loss': 1.1187, 'grad_norm': 1.816733956336975, 'learning_rate': 1.959682345754429e-05, 'epoch': 0.2}


  5%|▌         | 86/1688 [05:36<1:49:59,  4.12s/it]

  5%|▌         | 87/1688 [05:40<1:47:15,  4.02s/it]

  5%|▌         | 88/1688 [05:43<1:44:19,  3.91s/it]

  5%|▌         | 89/1688 [05:47<1:43:49,  3.90s/it]

  5%|▌         | 90/1688 [05:51<1:41:58,  3.83s/it]

  5%|▌         | 91/1688 [05:55<1:41:35,  3.82s/it]

  5%|▌         | 92/1688 [05:59<1:42:35,  3.86s/it]

  6%|▌         | 93/1688 [06:02<1:41:32,  3.82s/it]

  6%|▌         | 94/1688 [06:06<1:43:05,  3.88s/it]
{'loss': 1.7923, 'grad_norm': 2.680215835571289, 'learning_rate': 1.9474648747709226e-05, 'epoch': 0.22}

  6%|▌         | 95/1688 [06:10<1:41:12,  3.81s/it]


  6%|▌         | 97/1688 [06:18<1:45:04,  3.96s/it]
{'loss': 0.9137, 'grad_norm': 1.8026243448257446, 'learning_rate': 1.9437996334758705e-05, 'epoch': 0.23}

  6%|▌         | 98/1688 [06:22<1:44:17,  3.94s/it]


  6%|▌         | 100/1688 [06:29<1:38:32,  3.72s/it]

  6%|▌         | 101/1688 [06:33<1:38:37,  3.73s/it]

  6%|▌         | 102/1688 [06:37<1:39:57,  3.78s/it]

  6%|▌         | 103/1688 [06:41<1:40:51,  3.82s/it]

  6%|▌         | 104/1688 [06:44<1:40:10,  3.79s/it]

  6%|▌         | 105/1688 [06:48<1:39:32,  3.77s/it]
{'loss': 0.8984, 'grad_norm': 1.7200746536254883, 'learning_rate': 1.9340256566890655e-05, 'epoch': 0.25}


  6%|▋         | 107/1688 [06:55<1:37:48,  3.71s/it]

  6%|▋         | 108/1688 [06:59<1:38:43,  3.75s/it]

  6%|▋         | 109/1688 [07:03<1:37:32,  3.71s/it]

  7%|▋         | 110/1688 [07:07<1:42:32,  3.90s/it]

  7%|▋         | 111/1688 [07:11<1:40:51,  3.84s/it]

  7%|▋         | 112/1688 [07:15<1:39:42,  3.80s/it]

  7%|▋         | 113/1688 [07:18<1:40:04,  3.81s/it]

  7%|▋         | 114/1688 [07:23<1:42:27,  3.91s/it]

  7%|▋         | 115/1688 [07:26<1:41:07,  3.86s/it]
{'loss': 0.8248, 'grad_norm': 5.37086820602417, 'learning_rate': 1.921808185705559e-05, 'epoch': 0.27}

  7%|▋         | 116/1688 [07:30<1:40:34,  3.84s/it]


  7%|▋         | 118/1688 [07:37<1:38:35,  3.77s/it]

  7%|▋         | 119/1688 [07:42<1:40:44,  3.85s/it]

  7%|▋         | 120/1688 [07:45<1:40:34,  3.85s/it]

  7%|▋         | 121/1688 [07:49<1:39:49,  3.82s/it]

  7%|▋         | 122/1688 [07:53<1:39:46,  3.82s/it]

  7%|▋         | 123/1688 [07:57<1:39:22,  3.81s/it]

  7%|▋         | 124/1688 [08:01<1:39:06,  3.80s/it]

  7%|▋         | 125/1688 [08:04<1:40:00,  3.84s/it]

  7%|▋         | 126/1688 [08:08<1:41:00,  3.88s/it]
{'loss': 1.1876, 'grad_norm': 1.530632495880127, 'learning_rate': 1.9083689676237022e-05, 'epoch': 0.3}


  8%|▊         | 128/1688 [08:16<1:42:39,  3.95s/it]

  8%|▊         | 129/1688 [08:20<1:42:45,  3.95s/it]

  8%|▊         | 130/1688 [08:24<1:43:44,  4.00s/it]

  8%|▊         | 131/1688 [08:29<1:45:30,  4.07s/it]

  8%|▊         | 132/1688 [08:33<1:45:06,  4.05s/it]

  8%|▊         | 133/1688 [08:36<1:42:08,  3.94s/it]
{'loss': 1.1495, 'grad_norm': 1.6668986082077026, 'learning_rate': 1.8998167379352474e-05, 'epoch': 0.31}

  8%|▊         | 134/1688 [08:40<1:41:04,  3.90s/it]

  8%|▊         | 135/1688 [08:44<1:39:27,  3.84s/it]

  8%|▊         | 136/1688 [08:48<1:39:43,  3.86s/it]

  8%|▊         | 137/1688 [08:52<1:40:45,  3.90s/it]