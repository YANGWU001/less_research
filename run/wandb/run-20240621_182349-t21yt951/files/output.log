  0%|          | 0/4 [00:00<?, ?it/s]/home/ywu19/anaconda3/envs/less/lib/python3.10/site-packages/transformers/data/data_collator.py:646: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:274.)
  batch["labels"] = torch.tensor(batch["labels"], dtype=torch.int64)
{'loss': 3.1584, 'grad_norm': 6.829956531524658, 'learning_rate': 2e-05, 'epoch': 1.0}
 25%|██▌       | 1/4 [00:03<00:11,  3.69s/it][INFO|trainer.py:3410] 2024-06-21 18:23:58,289 >> Saving model checkpoint to ../out/llama2-7b-p0.0001-lora-seed3/checkpoint-1
[INFO|tokenization_utils_base.py:2513] 2024-06-21 18:24:00,144 >> tokenizer config file saved in ../out/llama2-7b-p0.0001-lora-seed3/checkpoint-1/tokenizer_config.json
[INFO|tokenization_utils_base.py:2522] 2024-06-21 18:24:00,148 >> Special tokens file saved in ../out/llama2-7b-p0.0001-lora-seed3/checkpoint-1/special_tokens_map.json
 50%|█████     | 2/4 [00:09<00:09,  4.84s/it]
{'loss': 0.7933, 'grad_norm': 2.8274741172790527, 'learning_rate': 1.3333333333333333e-05, 'epoch': 1.19}
 75%|███████▌  | 3/4 [00:11<00:03,  3.82s/it][INFO|trainer.py:3410] 2024-06-21 18:24:06,538 >> Saving model checkpoint to ../out/llama2-7b-p0.0001-lora-seed3/checkpoint-3
[INFO|tokenization_utils_base.py:2513] 2024-06-21 18:24:08,175 >> tokenizer config file saved in ../out/llama2-7b-p0.0001-lora-seed3/checkpoint-3/tokenizer_config.json
[INFO|tokenization_utils_base.py:2522] 2024-06-21 18:24:08,179 >> Special tokens file saved in ../out/llama2-7b-p0.0001-lora-seed3/checkpoint-3/special_tokens_map.json
{'loss': 1.1936, 'grad_norm': 3.406169891357422, 'learning_rate': 0.0, 'epoch': 2.37}
100%|██████████| 4/4 [00:17<00:00,  4.33s/it][INFO|trainer.py:3410] 2024-06-21 18:24:11,638 >> Saving model checkpoint to ../out/llama2-7b-p0.0001-lora-seed3/checkpoint-4
[INFO|tokenization_utils_base.py:2513] 2024-06-21 18:24:13,179 >> tokenizer config file saved in ../out/llama2-7b-p0.0001-lora-seed3/checkpoint-4/tokenizer_config.json
[INFO|tokenization_utils_base.py:2522] 2024-06-21 18:24:13,181 >> Special tokens file saved in ../out/llama2-7b-p0.0001-lora-seed3/checkpoint-4/special_tokens_map.json
{'train_runtime': 26.575, 'train_samples_per_second': 4.064, 'train_steps_per_second': 0.151, 'train_loss': 1.8286516517400742, 'epoch': 2.37}
[INFO|trainer.py:2329] 2024-06-21 18:24:15,237 >>
Training completed. Do not forget to share your model on huggingface.co/models =)
100%|██████████| 4/4 [00:20<00:00,  5.16s/it]
[INFO|trainer.py:3410] 2024-06-21 18:24:15,241 >> Saving model checkpoint to ../out/llama2-7b-p0.0001-lora-seed3
***** train metrics *****
  epoch                    =     2.3704
  total_flos               =   812459GF
  train_loss               =     1.8287
  train_runtime            = 0:00:26.57
  train_samples            =         27
  train_samples_per_second =      4.064
  train_steps_per_second   =      0.151
[INFO|tokenization_utils_base.py:2513] 2024-06-21 18:24:16,449 >> tokenizer config file saved in ../out/llama2-7b-p0.0001-lora-seed3/tokenizer_config.json
[INFO|tokenization_utils_base.py:2522] 2024-06-21 18:24:16,454 >> Special tokens file saved in ../out/llama2-7b-p0.0001-lora-seed3/special_tokens_map.json