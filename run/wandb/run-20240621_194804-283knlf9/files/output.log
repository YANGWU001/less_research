  0%|          | 0/4 [00:00<?, ?it/s]/home/ywu19/anaconda3/envs/less/lib/python3.10/site-packages/transformers/data/data_collator.py:646: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:274.)
  batch["labels"] = torch.tensor(batch["labels"], dtype=torch.int64)
 25%|██▌       | 1/4 [00:00<00:02,  1.01it/s][INFO|trainer.py:3410] 2024-06-21 19:48:09,095 >> Saving model checkpoint to ../out/llama2-7b-less-mmlu-p-lora/checkpoint-1
{'loss': 0.896, 'grad_norm': 6.2647013664245605, 'learning_rate': 2e-05, 'epoch': 1.0}
[INFO|tokenization_utils_base.py:2513] 2024-06-21 19:48:10,336 >> tokenizer config file saved in ../out/llama2-7b-less-mmlu-p-lora/checkpoint-1/tokenizer_config.json
[INFO|tokenization_utils_base.py:2522] 2024-06-21 19:48:10,339 >> Special tokens file saved in ../out/llama2-7b-less-mmlu-p-lora/checkpoint-1/special_tokens_map.json
 50%|█████     | 2/4 [00:04<00:05,  2.59s/it][INFO|trainer.py:3410] 2024-06-21 19:48:12,806 >> Saving model checkpoint to ../out/llama2-7b-less-mmlu-p-lora/checkpoint-2
{'loss': 0.896, 'grad_norm': 6.315030097961426, 'learning_rate': 1.3333333333333333e-05, 'epoch': 2.0}
[INFO|tokenization_utils_base.py:2513] 2024-06-21 19:48:14,077 >> tokenizer config file saved in ../out/llama2-7b-less-mmlu-p-lora/checkpoint-2/tokenizer_config.json
[INFO|tokenization_utils_base.py:2522] 2024-06-21 19:48:14,081 >> Special tokens file saved in ../out/llama2-7b-less-mmlu-p-lora/checkpoint-2/special_tokens_map.json
{'loss': 0.6291, 'grad_norm': 5.364523887634277, 'learning_rate': 6.666666666666667e-06, 'epoch': 3.0}
 75%|███████▌  | 3/4 [00:08<00:03,  3.10s/it][INFO|trainer.py:3410] 2024-06-21 19:48:16,517 >> Saving model checkpoint to ../out/llama2-7b-less-mmlu-p-lora/checkpoint-3
[INFO|tokenization_utils_base.py:2513] 2024-06-21 19:48:17,509 >> tokenizer config file saved in ../out/llama2-7b-less-mmlu-p-lora/checkpoint-3/tokenizer_config.json
[INFO|tokenization_utils_base.py:2522] 2024-06-21 19:48:17,511 >> Special tokens file saved in ../out/llama2-7b-less-mmlu-p-lora/checkpoint-3/special_tokens_map.json
{'loss': 0.5297, 'grad_norm': 2.6807382106781006, 'learning_rate': 0.0, 'epoch': 4.0}
100%|██████████| 4/4 [00:11<00:00,  3.20s/it][INFO|trainer.py:3410] 2024-06-21 19:48:19,876 >> Saving model checkpoint to ../out/llama2-7b-less-mmlu-p-lora/checkpoint-4
[INFO|tokenization_utils_base.py:2513] 2024-06-21 19:48:21,336 >> tokenizer config file saved in ../out/llama2-7b-less-mmlu-p-lora/checkpoint-4/tokenizer_config.json
[INFO|tokenization_utils_base.py:2522] 2024-06-21 19:48:21,339 >> Special tokens file saved in ../out/llama2-7b-less-mmlu-p-lora/checkpoint-4/special_tokens_map.json
[INFO|trainer.py:2329] 2024-06-21 19:48:23,499 >>
Training completed. Do not forget to share your model on huggingface.co/models =)
100%|██████████| 4/4 [00:15<00:00,  3.85s/it]
[INFO|trainer.py:3410] 2024-06-21 19:48:23,503 >> Saving model checkpoint to ../out/llama2-7b-less-mmlu-p-lora
{'train_runtime': 20.4868, 'train_samples_per_second': 0.976, 'train_steps_per_second': 0.195, 'train_loss': 0.7376794517040253, 'epoch': 4.0}
[INFO|tokenization_utils_base.py:2513] 2024-06-21 19:48:24,907 >> tokenizer config file saved in ../out/llama2-7b-less-mmlu-p-lora/tokenizer_config.json
[INFO|tokenization_utils_base.py:2522] 2024-06-21 19:48:24,914 >> Special tokens file saved in ../out/llama2-7b-less-mmlu-p-lora/special_tokens_map.json
***** train metrics *****
  epoch                    =        4.0
  total_flos               =   112863GF
  train_loss               =     0.7377
  train_runtime            = 0:00:20.48
  train_samples            =          5
  train_samples_per_second =      0.976
  train_steps_per_second   =      0.195